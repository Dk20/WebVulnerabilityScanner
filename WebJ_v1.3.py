from bs4 import BeautifulSoup
import urllib.request, urllib.error, urllib.parse
import urllib.parse
import re
import requests
from pprint import pprint
#pp = pprint.PrettyPrinter(indent=4,compact=False,width=1)
#from termcolor import colored
#import ssl

l=list()
#print ("         "+"Secure ")
#url = 'http://example.com/?q=abc&p=123'

def check(url):
        #gcontext = ssl.SSLContext(ssl.PROTOCOL_TLSv1)  # Only for gangstars
	try:
		page=urllib.request.urlopen(url)
		parts = url.split("/")
		#print (parts[-1])
		if (parts[-1]=="admin"):
			print("         "+"         "+"Admin label vulnerability!!!!")
		elif (parts[-1]=="login"):
			print("         "+"         "+"Login label vulnerability!!")
		else :
			soup = BeautifulSoup(page,"html.parser")
			for form in soup.findAll('form'):
					print("         "+"         Form Method:",form.get('method'))
					if(form.get('method')=='GET' or form.get('method')=='get'):
						print("         "+"         "+"request sent via get method... data Vulnerable!!!")
			try:
				result=requests.get(url+"'")
				print("         "+"         "+"On priliminary sql injection status code :",result.status_code)
				print("         "+"         ")
				print("         "+"         ")
				print("         "+"         "+"On priliminary sql injection response header from server :")
				print("         "+"         ")
				#print("         "+"         ",result.headers)
				pprint(result.headers,width=1,indent=3)
				print("         "+"         ")
				print("         "+"         ")
				print("         "+"         ")
				print("         "+"         ")
				a=[404,500,408,302]
				if(result.status_code in a):
						print("         "+"         "+"God Bless this site, its partially or 100% injectable.....!!")
			except:
				print("         "+"         "+"Request denied, This page is safe...... :-)")
	except:
		print("         "+"         "+"This site does not have an ssl certificate... Vulnerable!!!")
		print("         "+"         "+"Exiting..........")

'''
def xss(url):
    try:

             request = requests.get(url)
             parseHTML = BeautifulSoup(request.text, 'html.parser')

             htmlForm = parseHTML.form

             formName = htmlForm['name']

             print "Form name: " + formName


             inputs = htmlForm.find_all('input')

             inputFieldNames = []

             for items in inputs:
                if items.has_attr('name'):
                 inputFieldNames.append(items['name'])

             print inputFieldNames

             import mechanize

             formSubmit = mechanize.Browser()

             formSubmit.open("http://check.cyberpersons.com/crossSiteCheck.html")

             formSubmit.select_form(formName)


             payLoad = '&lt;script&gt;alert("vulnerable");&lt;/script&gt;'

            # First field is always the payload, you can select anyfield for payload
            # But that don't edit it later.

             formSubmit.form[inputFieldNames[0]] = payLoad

             for i in range(1,len(inputFieldNames)):
                formSubmit.form[inputFieldNames[i]] = "Customized text"


             formSubmit.submit()

             finalResult = formSubmit.response().read()

             if finalResult.find('&lt;script&gt;')&gt;=0:
                print "Application is vulnerable"
             else:
                print "You are in good hands"
    except:
            print "         "+"         "+"cross_site not working...."


'''

url=input("Enter the url :")
l.append(url)
j=0
#gcontext = ssl.SSLContext(ssl.PROTOCOL_TLSv1)  # Only for gangstars
try:
	while(j<=3):
		for i in range(0,len(l)):
			html_page = urllib.request.urlopen(str(l[i]))
			soup = BeautifulSoup(html_page,"html.parser")
			sites_added = 0
			for link in soup.findAll('a', attrs={'href': re.compile("^http://")}):
				p=link.get('href')
				parsed_url=urllib.parse.urlparse(url)
				if bool(parsed_url.netloc):     #just checking before adding
					if bool(parsed_url.scheme):
						l.append(p)
						#print(l)
					else:
						parsed_url = parsed_url._replace(**{"scheme": "http"})
						l.append(parsed_url.geturl())
						#print(parsed_url.geturl())
				sites_added+=1
			print("Depth : "+str(j))
			i=0
			while(i<=(len(l)-sites_added)):
				check_this=l.pop()
				print("         "+"Iter "+str(i)+"  :"+check_this)
				#check_this=l.pop()
				i+=1
				check(check_this)
			j=j+1
			if(j>3):
				break
except:
	print("This website is not Crawlable. Please try another one. ")

#for link in soup.findAll('a', attrs={'href': re.compile("^http://")}):
#    print link.get('href')
